<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SysML@ICL</title><link>https://sysml-icl.github.io/</link><atom:link href="https://sysml-icl.github.io/index.xml" rel="self" type="application/rss+xml"/><description>SysML@ICL</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 16 Jun 2022 10:30:00 +0000</lastBuildDate><image><url>https://sysml-icl.github.io/media/logo_hu0e4a96b537c97ee56dff564995bc792a_102878_300x300_fit_lanczos_3.png</url><title>SysML@ICL</title><link>https://sysml-icl.github.io/</link></image><item><title>Seminar #1 - TBD</title><link>https://sysml-icl.github.io/event/seminar1/</link><pubDate>Thu, 16 Jun 2022 10:30:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/seminar1/</guid><description>&lt;p>TBD Placeholder&lt;/p>
&lt;hr>
&lt;h5 style="text-align: center;">Share&lt;/h5></description></item><item><title>Reading Group Session #1</title><link>https://sysml-icl.github.io/event/reading1/</link><pubDate>Thu, 09 Jun 2022 10:30:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/reading1/</guid><description>&lt;h1 id="papers">Papers&lt;/h1>
&lt;p>Papers covered will be:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://ieeexplore.ieee.org/iel7/71/9497774/09472938.pdf" target="_blank" rel="noopener">vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training&lt;/a>, TPDS'21&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/pdf/2201.12023" target="_blank" rel="noopener">Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning&lt;/a>, OSDI'22&lt;/li>
&lt;/ol>
&lt;h1 id="discussion-notes">Discussion Notes&lt;/h1>
&lt;p>Come back later when this section is populated with the discussion.&lt;/p>
&lt;h2 id="vpipe">vPipe&lt;/h2>
&lt;h2 id="alpa">Alpa&lt;/h2>
&lt;hr>
&lt;h5 style="text-align: center;">Share&lt;/h5></description></item><item><title>Parallel Automatic Differentiation Talk</title><link>https://sysml-icl.github.io/event/parallel-ad/</link><pubDate>Fri, 13 May 2022 11:00:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/parallel-ad/</guid><description>&lt;p>&lt;strong>Talk Title:&lt;/strong> Enzyme: High-Performance, Cross-Language, and Parallel Automatic Differentiation&lt;/p>
&lt;p>&lt;a href="https://imperial-ac-uk.zoom.us/j/97378427420?pwd=VENub3h5eVJscXVTbDk5cG9sVGFMdz09" target="_blank" rel="noopener">Zoom Link&lt;/a>&lt;/p>
&lt;h2 id="author-bio">Author Bio&lt;/h2>
&lt;p>William Moses is a Ph.D. Candidate at MIT, where he also received his M.Eng in electrical engineering and computer science (EECS) and B.S. in EECS and physics. William&amp;rsquo;s research involves creating compilers and program representations that enable performance and use-case portability, thus enabling non-experts to leverage the latest in high-performance computing and ML. He is known as the lead developer of Enzyme (NeurIPS &amp;lsquo;20, SC &amp;lsquo;21), an automatic differentiation tool for LLVM capable of differentiating code in a variety of languages, after optimization, and for a variety of architectures and the lead developer of Polygeist (PACT &amp;lsquo;21), a polyhedral compiler and C++ frontend for MLIR. He has also worked on the Tensor Comprehensions framework for synthesizing high-performance GPU kernels of ML code, the Tapir compiler for parallel programs (best paper at PPoPP &amp;lsquo;17), and compilers that use machine learning to better optimize. He is a recipient of the U.S. Department of Energy Computational Science Graduate Fellowship and the Karl Taylor Compton Prize, MIT&amp;rsquo;s highest student award.&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Automatic differentiation (AD) is key to training neural networks, Bayesian inference, and scientific computing. Applying these techniques requires rewriting code in a specific machine learning framework or manually providing derivatives. This talk presents Enzyme, a high-performance automatic differentiation compiler plugin for the low-level virtual machine (LLVM) compiler capable of synthesizing gradients of programs expressed in the LLVM intermediate representation (IR). Enzyme differentiates programs in any language whose compiler targets LLVM, including C/C++, Fortran, Julia, Rust, Swift, etc., thereby providing native AD capabilities in these languages with state-of-the-art performance. Unlike traditional tools, Enzyme performs AD on optimized IR. On a combined machine-learning and scientific computing benchmark suite, AD on optimized IR achieves a geometric mean speedup of 4.2x over AD on IR before optimization. This talk will also include work that makes Enzyme the first fully automatic reverse-mode AD tool to generate gradients of existing GPU kernels. This includes new GPU and AD-specific compiler optimizations, and an algorithm ensuring correctness of high-performance parallel gradient computations. We provide a detailed evaluation of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs.&lt;/p>
&lt;hr>
&lt;h5 style="text-align: center;">Share&lt;/h5></description></item><item><title>HiPEDS Centre hosting talk on Parallel Auto-Differentiation</title><link>https://sysml-icl.github.io/post/13-05-2022-enzyme/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://sysml-icl.github.io/post/13-05-2022-enzyme/</guid><description>&lt;p>!(Link)[https://imperial-ac-uk.zoom.us/j/97378427420?pwd=VENub3h5eVJscXVTbDk5cG9sVGFMdz09]&lt;/p></description></item><item><title/><link>https://sysml-icl.github.io/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sysml-icl.github.io/contact/</guid><description/></item><item><title/><link>https://sysml-icl.github.io/people/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sysml-icl.github.io/people/</guid><description/></item></channel></rss>