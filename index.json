[{"authors":null,"categories":null,"content":"Pedro Silvestre is a PhD student in the Large-Scale Data \u0026amp; Systems Group at Imperial College London, under the supervision of Dr. Peter Pietzuch. His research at Imperial focuses on the interplay between Dataflow Systems and novel Deep Learning workloads, in particular Deep Reinforcement Learning. Before Imperial, he was a Research Engineer at the TU Delft’s Web Information Systems Group working on Clonos.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sysml.doc.ic.ac.uk/author/pedro-f.-silvestre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pedro-f.-silvestre/","section":"authors","summary":"Pedro Silvestre is a PhD student in the Large-Scale Data \u0026 Systems Group at Imperial College London, under the supervision of Dr. Peter Pietzuch. His research at Imperial focuses on the interplay between Dataflow Systems and novel Deep Learning workloads, in particular Deep Reinforcement Learning.","tags":null,"title":"Pedro F. Silvestre","type":"authors"},{"authors":null,"categories":null,"content":"Marcel Wagenlaender is a PhD student in the Large-Scale Data \u0026amp; Systems Group at Imperial College London, under the supervision of Dr. Peter Pietzuch. His research at Imperial focuses on Systems for Machine Learning. Before joining the LSDS group, he did his Bachelor and Master at the Technical University of Munich\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7aa5ae707052fee850418ad8f7c62a79","permalink":"https://sysml.doc.ic.ac.uk/author/marcel-wagenlaender/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/marcel-wagenlaender/","section":"authors","summary":"Marcel Wagenlaender is a PhD student in the Large-Scale Data \u0026 Systems Group at Imperial College London, under the supervision of Dr. Peter Pietzuch. His research at Imperial focuses on Systems for Machine Learning.","tags":null,"title":"Marcel Wagenlaender","type":"authors"},{"authors":[],"categories":["Seminar"],"content":" Join Zoom Session! Add to Calendar Speaker Bio Roger Waleffe is a PhD student at the University of Wisconsin-Madison working under the supervision of Theodoros Rekatsinas. His work focuses on the intersection of systems and algorithmic challenges for IO-aware training of large-scale ML models. Roger is a lead in the Marius project (https://marius-project.org/) which aims to make the use of Graph Neural Networks and Graph Embeddings over billion-scale graphs easier, faster, and cheaper. Roger holds a B.S. and M.S. in computer science from UW-Madison and is a recipient of the UW-Madison departmental graduate research fellowship and Goldwater scholarship.\n","date":1681999200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681999200,"objectID":"44dd8762b13c13abb9856121866d27da","permalink":"https://sysml.doc.ic.ac.uk/event/seminar8/","publishdate":"2023-02-22T00:00:00Z","relpermalink":"/event/seminar8/","section":"event","summary":"Roger Waleffe will present his work MariusGNN: Training Graph Neural Networks over Billion-Scale Graphs on a Single Machine (EuroSys'23)","tags":["GNN","DL Training","Pipelining","Graphs","Big Data","GNN Training","Graph Neural Network","Memory Hierarchy","Scale-up"],"title":"Seminar #8 - Roger Waleffe - MariusGNN (EuroSys'23)","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Zoom Session! Add to Calendar Speaker Bio Fan Mo is a Senior Researcher at Trustworthy AI Lab, Huawei. Before that, he completed his PhD at Imperial College London in 2022, advised by Professor Hamed Haddadi. He also worked for a short time in Nokia Bell Labs, Arm Research, and Telefónica Research. His research focuses on data privacy and trustworthiness in machine learning at the edge, machine learning in confidential computing, and distributed machine learning on lightweight mobile/IoT devices.\n","date":1675951200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675951200,"objectID":"3685aac938404c9882ecb703bd77be4d","permalink":"https://sysml.doc.ic.ac.uk/event/seminar7/","publishdate":"2023-01-10T00:00:00Z","relpermalink":"/event/seminar7/","section":"event","summary":"Fan Mo will present his work PPFL: Privacy-preserving Federated Learning with Trusted Execution Environments (MobiSys'21)","tags":["Federated Learning","Trusted Execution Environments","Privacy","Intel SGX","Adversarial Attacks"],"title":"Seminar #7 - Fan Mo - PPFL (MobiSys'21)","type":"event"},{"authors":[],"categories":["Reading Group"],"content":"Pathways Join Zoom Session! Paper Link, SysML\u0026#39;22\nPaper Abstract:\nWe present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (~100% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.\nDiscussion Notes TBD\n","date":1674136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674136800,"objectID":"29e28cd871d64ee9c03f9351d22ca984","permalink":"https://sysml.doc.ic.ac.uk/event/reading3/","publishdate":"2023-01-10T00:00:00Z","relpermalink":"/event/reading3/","section":"event","summary":"Pathways: Asynchronous distributed dataflow for ML (MLSys'22)","tags":["Dataflow","Distributed","Asynchronous","DL Training","Accelerators","Gang-Scheduling","Networking","SPMD","Pipeline Parallelism","Data Parallelism","Model Parallelism","TPU"],"title":"Reading Group Session #3","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Zoom Session! Add to Calendar Speaker Bio Michael is a final year PhD student in the Computer Science Department of Carnegie Mellon University advised by George Amvrosiadis and Virginia Smith. He received his bachelor’s degree from Georgia Institute of Technology, and his research was supported by an NDSEG fellowship. He is broadly interested in computer systems and is currently working on systems for machine learning, with a particular emphasis on data pipelines (e.g., data augmentations, I/O formats, and pipeline design).\n","date":1665669600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665669600,"objectID":"84ddb7a06ebcd4c062be359a05f21b73","permalink":"https://sysml.doc.ic.ac.uk/event/seminar6/","publishdate":"2022-09-15T00:00:00Z","relpermalink":"/event/seminar6/","section":"event","summary":"Michael Kuchnik will present his work on Plumber (MLSys'22)","tags":["DL Training","Input Pipelines","Optimization","Analytical Modelling","Data Parallelism"],"title":"Seminar #6 - Michael Kuchnik - Plumber (MLSys'22)","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Zoom Session! Add to Calendar Speaker Bio Colin Unger is a second year PhD student at Stanford advised by Alex Aiken. He received his bachelor’s degree from UC Santa Barbara, where he worked with Giovanni Vigna and Christopher Kruegel on binary analysis. He is broadly interested in compilers, program analysis, and optimization, especially in emerging applications, and is currently focused on hardware-aware optimization of deep learning workloads.\n","date":1662633e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662633e3,"objectID":"8c8e3d16558e2a5c4304bd8c40935424","permalink":"https://sysml.doc.ic.ac.uk/event/seminar5/","publishdate":"2022-07-23T00:00:00Z","relpermalink":"/event/seminar5/","section":"event","summary":"Colin Unger will present his work on Unity (OSDI'22)","tags":["Compilers","DL Training","Pipeline Parallelism","Tensor Parallelism","Data Parallelism","Algebraic Optimization","Verification","Strategy Search"],"title":"Seminar #5 - Colin Unger - Unity (OSDI'22)","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Zoom Session! Add to Calendar Speaker Bio Artem Artemev is at his second year PhD research in Imperial College London, with Mark van der Wilk as his supervisor. Before starting PhD, Artem has worked as Machine Learning Researcher and Engineer for start-up companies. His primary interest lies in machine learning theory and approximate Bayesian inference, particularly non-parametric machine learning methods like Gaussian processes. His recent work addresses the scalability of Gaussian processes while retaining the predictive properties of that model when approximations are involved.\n","date":1662028200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662028200,"objectID":"d2c2fc6d5f11d79dca9f379cd8627ff2","permalink":"https://sysml.doc.ic.ac.uk/event/seminar4/","publishdate":"2022-08-25T00:00:00Z","relpermalink":"/event/seminar4/","section":"event","summary":"Artem Artemev will present his work on eXLA","tags":["Domain Specific Languages","Compilers","Dataflow","XLA","JAX","Memory Safety","Memory-Efficiency","Algorithms"],"title":"Seminar #4 - Artem Artemev - Memory Safe Computations with XLA Compiler ","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Zoom Session! Add to Calendar Speaker Bio Abhinav Jangda is a PhD student at the University of Massachusetts Amherst and will join Microsoft Research in Fall 2022. His research focuses on developing programming language abstractions and compilation techniques to help programmers leverage large scale systems efficiently. His work was invited for an article in USENIX :login;, has received an ACM SIGPLAN Distinguished Paper Award at OOPSLA, and a Best Paper Award at PACT.\n","date":1659004200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659004200,"objectID":"da8f693d8317265eeb3449027363f1da","permalink":"https://sysml.doc.ic.ac.uk/event/seminar3/","publishdate":"2022-06-21T00:00:00Z","relpermalink":"/event/seminar3/","section":"event","summary":"Abhinav Jangda will present his work on CoCoNet (ASPLOS'22)","tags":["Domain Specific Languages","Compilers","DL Training","Communication","Optimization","Stencil Computations","Pipeline Parallelism","Tensor Parallelism","Data Parallelism","MPI","Collective Communication"],"title":"Seminar #3 - Abhinav Jangda - CoCoNet (ASPLOS'22)","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Session! Speaker Bio Alexander Renz-Wieland is a PhD student working on large-scale machine learning student in the Database Systems and Information Management (DIMA) group at Technische Universität Berlin since September 2017. He is supervised by Volker Markl and Rainer Gemulla (Universität Mannheim). Prior to his PhD, he completed a M. Sc. Business Informatics with a specialization in Data and Web Science at Universität Mannheim, with a semester at VU Amsterdam (with courses from CWI). In his Master’s thesis, he worked on scalable sequential pattern mining.\n","date":1658399400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658399400,"objectID":"d50c9c844d14a9f344419994061ae819","permalink":"https://sysml.doc.ic.ac.uk/event/seminar2/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/event/seminar2/","section":"event","summary":"Alexander Renz-Wieland will present his work on Adaptive Parameter Servers and NuPS (SIGMOD'22)","tags":["Parameter Servers","DL Training","Parameter Management","Sampling","Skew","Non-uniform Parameter Access"],"title":"Seminar #2 - Alexander Renz-Wieland - NuPS (SIGMOD'22)","type":"event"},{"authors":[],"categories":["Seminar"],"content":" Join Session! Speaker Bio Davis Blalock is a research scientist at MosaicML. He completed his PhD at MIT, advised by Professor John Guttag. His primary work is designing efficient machine learning algorithms. He received his M.S. from MIT and his B.S. from the University of Virginia. He is a Qualcomm Innovation Fellow, NSF Graduate Research Fellow, and Barry M. Goldwater Scholar.\n","date":1657794600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657794600,"objectID":"4040f60f75dd9e1a75f4fbb862e89a52","permalink":"https://sysml.doc.ic.ac.uk/event/seminar1/","publishdate":"2022-06-21T00:00:00Z","relpermalink":"/event/seminar1/","section":"event","summary":"Davis Blalock will present his work on MADDNESS (ICML'21)","tags":["DL Training","Optimization","Approximate Methods","ML for MLSys","Quantization","GEMM"],"title":"Seminar #1 - Davis Blalock - MADDNESS (ICML'21)","type":"event"},{"authors":[],"categories":["Reading Group"],"content":" Join Session! Papers A Generalist Agent, arXiv\u0026#39;22\nPaper Abstract: Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.\nDiscussion Notes Gato is a new take on how to achieve multi-task agents based on autoregressive modeling of sequence data.\nThis paper was perhaps a bit out of scope for a SysML group, but we came up with a few future directions from the reading.\nInput pipeline The input pipeline in Gato is far more complex than a conventional Supervised Learning input pipeline\nThe inputs come from a large number of datasets (604, one per task) all which have large corpora. Are available sampling mechanisms able to scale to these dimensions?\nFurthermore, on each iteration it requires tokenization, linearization and embedding of multi-modal inputs such that a batch containing a mix of tasks is formed. While images are embedded using a ResNet block, text, actions and observations are embedded through a learned embedding. Are existing libraries such as tf.data capable of such complex input pipelines? Can they maintain good throughput with these unbalanced tasks?\nDemocratizing Fine-Tuning A common pattern nowadays seems to be that large research groups will pre-train extremely large general models (language models) and other groups will then fine-tune them to achieve good performance on a specific task (fine-tuning). How is the workload of fine-tuning different from pre-training? Would it be possible to democratize fine-tuning such that it can be done on a laptop?\nAttention, Memory and Context Windows To maintain a memory of the recent past, Gato uses a sliding window of recent observations and actions. To achieve the long-term planning needed for general RL, an infinitely sized window would be needed. However, growing the window size in current Transformer implementations would grow memory and compute requirements too. Are Transformers the correct approach to RL? Could transformers be augmented with a long-term memory component? Or could their training systems be fundamentally altered such that they can handle larger windows without growing memory requirements?\n","date":1655980200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655980200,"objectID":"3162dd10a948330d819d59244410b848","permalink":"https://sysml.doc.ic.ac.uk/event/reading2/","publishdate":"2022-06-19T00:00:00Z","relpermalink":"/event/reading2/","section":"event","summary":"Gato (arXiv'22)","tags":["Transformers","DL Training","Multi-Modal","Multi-Task","AGI","Embeddings"],"title":"Reading Group Session #2","type":"event"},{"authors":[],"categories":["Reading Group"],"content":" Join Session! Papers Papers covered will be:\nvPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training, TPDS\u0026#39;21 Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning, OSDI\u0026#39;22 Discussion Notes In these discussion notes we attempt to summarize points made during the discussion on possible future directions.\nvPipe Online partitioning vPipes justification for the need for an online repartitioning algorithm is Neural Architecture Search (NAS). Very few systems are using online partitioning nowadays. The question is, is there a need for them? What are other motivating reasons for online partitioning?\nTo deal with failures To support elasticity NAS Dynamic networks In short, any source of dynamism in the training is justifiable. What are other sources of dynamism in training?\nPCIe Usage In vPipe PCIe is shared by swapping and inter-host activation communication. The algorithm which decides the swap-recompute plan attempts to fill all the PCIe bandwidth but ignores activation communication traffic. Is this optimal? Won’t this cause stalls by oversubscribing the PCIe bus? Still, vPipe prioritizes inter-host activation communication.\nAlpa Device Meshes Alpa maps computations to a 2D mesh of devices. This seems to be due to the fact that there are 2 layers in the device hierarchy, intra-host and inter-host, where communication intra-host is faster than inter-host. If there were a third level in this hierarchy, perhaps a 3D mesh of devices would make sense.\nBoth papers We noticed that both papers attempt to solve the NP-Complete task of parallelizing a computational graph across several devices for optimal performance. This task is too difficult to solve directly. To tackle this, both papers use a decomposition approach, instead solving two subproblems optimally:\nAlpa first uses inter-op parallelism, then intra-op parallelism on each stage. vPipe first uses inter-op parallelism, then computes a swap/recompute plan for each stage. Why can’t it be solved directly? Is it simply too large a search problem? Are we losing something by not solving the original problem optimally?\nHeterogeneous Clusters Heterogeneous clusters are a reality in several organizations which over time accumulate several generations of accelerators. Furthermore, CPUs are a largely unused resource that is available. Both papers, and other systems, generally target homogeneous clusters only. We believe there is space for novel systems targeting Heterogeneous clusters, though the problem of partitioning and parallelizing becomes more difficult.\n","date":1654770600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654770600,"objectID":"258c5f9607f6f7f6230750c1034748a3","permalink":"https://sysml.doc.ic.ac.uk/event/reading1/","publishdate":"2022-06-06T00:00:00Z","relpermalink":"/event/reading1/","section":"event","summary":"vPipe (TPDS'21) \u0026 Alpa (OSDI'22)","tags":["DL Training","Memory Management","Distributed","Compilers","Auto-parallelization","Pipeline Parallelism","Data Parallelism","Tensor Parallelism"],"title":"Reading Group Session #1","type":"event"},{"authors":[],"categories":["News"],"content":"Talk Title: Enzyme: High-Performance, Cross-Language, and Parallel Automatic Differentiation\nZoom Link\nAuthor Bio William Moses is a Ph.D. Candidate at MIT, where he also received his M.Eng in electrical engineering and computer science (EECS) and B.S. in EECS and physics. William’s research involves creating compilers and program representations that enable performance and use-case portability, thus enabling non-experts to leverage the latest in high-performance computing and ML. He is known as the lead developer of Enzyme (NeurIPS ‘20, SC ‘21), an automatic differentiation tool for LLVM capable of differentiating code in a variety of languages, after optimization, and for a variety of architectures and the lead developer of Polygeist (PACT ‘21), a polyhedral compiler and C++ frontend for MLIR. He has also worked on the Tensor Comprehensions framework for synthesizing high-performance GPU kernels of ML code, the Tapir compiler for parallel programs (best paper at PPoPP ‘17), and compilers that use machine learning to better optimize. He is a recipient of the U.S. Department of Energy Computational Science Graduate Fellowship and the Karl Taylor Compton Prize, MIT’s highest student award.\nAbstract Automatic differentiation (AD) is key to training neural networks, Bayesian inference, and scientific computing. Applying these techniques requires rewriting code in a specific machine learning framework or manually providing derivatives. This talk presents Enzyme, a high-performance automatic differentiation compiler plugin for the low-level virtual machine (LLVM) compiler capable of synthesizing gradients of programs expressed in the LLVM intermediate representation (IR). Enzyme differentiates programs in any language whose compiler targets LLVM, including C/C++, Fortran, Julia, Rust, Swift, etc., thereby providing native AD capabilities in these languages with state-of-the-art performance. Unlike traditional tools, Enzyme performs AD on optimized IR. On a combined machine-learning and scientific computing benchmark suite, AD on optimized IR achieves a geometric mean speedup of 4.2x over AD on IR before optimization. This talk will also include work that makes Enzyme the first fully automatic reverse-mode AD tool to generate gradients of existing GPU kernels. This includes new GPU and AD-specific compiler optimizations, and an algorithm ensuring correctness of high-performance parallel gradient computations. We provide a detailed evaluation of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs.\nShare ","date":1652439600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652439600,"objectID":"6fc08c749c0ebb9a7db0a7e8e1e7ddc1","permalink":"https://sysml.doc.ic.ac.uk/event/parallel-ad/","publishdate":"2022-05-11T00:00:00Z","relpermalink":"/event/parallel-ad/","section":"event","summary":"HiPEDS Centre hosting seminar on Parallel Automatic Differentiation by William Moses.","tags":["Automatic Differentiation","Compilers","GPU","LLVM"],"title":"Parallel Automatic Differentiation Talk","type":"event"},{"authors":null,"categories":null,"content":"!(Link)[https://imperial-ac-uk.zoom.us/j/97378427420?pwd=VENub3h5eVJscXVTbDk5cG9sVGFMdz09]\n","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"451c7d4292c928d3952b359b5c2ae138","permalink":"https://sysml.doc.ic.ac.uk/post/13-05-2022-enzyme/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/post/13-05-2022-enzyme/","section":"post","summary":"!(Link)[https://imperial-ac-uk.zoom.us/j/97378427420?pwd=VENub3h5eVJscXVTbDk5cG9sVGFMdz09]","tags":null,"title":"HiPEDS Centre hosting talk on Parallel Auto-Differentiation","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://sysml.doc.ic.ac.uk/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://sysml.doc.ic.ac.uk/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]