<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPU | SysML@ICL</title><link>https://sysml-icl.github.io/tag/gpu/</link><atom:link href="https://sysml-icl.github.io/tag/gpu/index.xml" rel="self" type="application/rss+xml"/><description>GPU</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 13 May 2022 11:00:00 +0000</lastBuildDate><image><url>https://sysml-icl.github.io/media/logo_hu0e4a96b537c97ee56dff564995bc792a_102878_300x300_fit_lanczos_3.png</url><title>GPU</title><link>https://sysml-icl.github.io/tag/gpu/</link></image><item><title>Parallel Automatic Differentiation Talk</title><link>https://sysml-icl.github.io/event/parallel-ad/</link><pubDate>Fri, 13 May 2022 11:00:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/parallel-ad/</guid><description>&lt;p>&lt;strong>Talk Title:&lt;/strong> Enzyme: High-Performance, Cross-Language, and Parallel Automatic Differentiation&lt;/p>
&lt;p>&lt;a href="https://imperial-ac-uk.zoom.us/j/97378427420?pwd=VENub3h5eVJscXVTbDk5cG9sVGFMdz09" target="_blank" rel="noopener">Zoom Link&lt;/a>&lt;/p>
&lt;h2 id="author-bio">Author Bio&lt;/h2>
&lt;p>William Moses is a Ph.D. Candidate at MIT, where he also received his M.Eng in electrical engineering and computer science (EECS) and B.S. in EECS and physics. William&amp;rsquo;s research involves creating compilers and program representations that enable performance and use-case portability, thus enabling non-experts to leverage the latest in high-performance computing and ML. He is known as the lead developer of Enzyme (NeurIPS &amp;lsquo;20, SC &amp;lsquo;21), an automatic differentiation tool for LLVM capable of differentiating code in a variety of languages, after optimization, and for a variety of architectures and the lead developer of Polygeist (PACT &amp;lsquo;21), a polyhedral compiler and C++ frontend for MLIR. He has also worked on the Tensor Comprehensions framework for synthesizing high-performance GPU kernels of ML code, the Tapir compiler for parallel programs (best paper at PPoPP &amp;lsquo;17), and compilers that use machine learning to better optimize. He is a recipient of the U.S. Department of Energy Computational Science Graduate Fellowship and the Karl Taylor Compton Prize, MIT&amp;rsquo;s highest student award.&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Automatic differentiation (AD) is key to training neural networks, Bayesian inference, and scientific computing. Applying these techniques requires rewriting code in a specific machine learning framework or manually providing derivatives. This talk presents Enzyme, a high-performance automatic differentiation compiler plugin for the low-level virtual machine (LLVM) compiler capable of synthesizing gradients of programs expressed in the LLVM intermediate representation (IR). Enzyme differentiates programs in any language whose compiler targets LLVM, including C/C++, Fortran, Julia, Rust, Swift, etc., thereby providing native AD capabilities in these languages with state-of-the-art performance. Unlike traditional tools, Enzyme performs AD on optimized IR. On a combined machine-learning and scientific computing benchmark suite, AD on optimized IR achieves a geometric mean speedup of 4.2x over AD on IR before optimization. This talk will also include work that makes Enzyme the first fully automatic reverse-mode AD tool to generate gradients of existing GPU kernels. This includes new GPU and AD-specific compiler optimizations, and an algorithm ensuring correctness of high-performance parallel gradient computations. We provide a detailed evaluation of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs.&lt;/p>
&lt;hr>
&lt;h5 style="text-align: center;">Share&lt;/h5></description></item></channel></rss>