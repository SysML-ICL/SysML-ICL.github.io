<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dataflow | SysML@ICL</title><link>https://sysml.doc.ic.ac.uk/tag/dataflow/</link><atom:link href="https://sysml.doc.ic.ac.uk/tag/dataflow/index.xml" rel="self" type="application/rss+xml"/><description>Dataflow</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 19 Jan 2023 10:30:00 +0000</lastBuildDate><image><url>https://sysml.doc.ic.ac.uk/media/logo_hu0e4a96b537c97ee56dff564995bc792a_102878_300x300_fit_lanczos_3.png</url><title>Dataflow</title><link>https://sysml.doc.ic.ac.uk/tag/dataflow/</link></image><item><title>Reading Group Session #3</title><link>https://sysml.doc.ic.ac.uk/event/reading3/</link><pubDate>Thu, 19 Jan 2023 10:30:00 +0000</pubDate><guid>https://sysml.doc.ic.ac.uk/event/reading3/</guid><description>&lt;h1 id="pathways">Pathways&lt;/h1>
&lt;p>&lt;a href="https://proceedings.mlsys.org/paper/2022/hash/98dce83da57b0395e163467c9dae521b-Abstract.html" target="_blank" rel="noopener">Paper Link&lt;/a>, SysML'22&lt;/p>
&lt;p>&lt;strong>Paper Abstract:&lt;/strong>&lt;/p>
&lt;p>We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (~100% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.&lt;/p>
&lt;h1 id="discussion-notes">Discussion Notes&lt;/h1>
&lt;p>TBD&lt;/p></description></item><item><title>Seminar #4 - Artem Artemev - Memory Safe Computations with XLA Compiler</title><link>https://sysml.doc.ic.ac.uk/event/seminar4/</link><pubDate>Thu, 01 Sep 2022 10:30:00 +0000</pubDate><guid>https://sysml.doc.ic.ac.uk/event/seminar4/</guid><description>&lt;ul class="cta-group">
&lt;li>
&lt;a href="https://imperial-ac-uk.zoom.us/j/98692000316?pwd=SnBNV21oQ2VGaGJDem51SVRlWG9pZz09" class="btn btn-primary px-3 py-3">Join Zoom Session!&lt;/a>
&lt;/li>
&lt;/ul>
&lt;div style="text-align: center;">
&lt;a title="Add to Calendar" class="addeventatc" data-id="Ji14783117" href="https://www.addevent.com/event/Ji14783117" target="_blank">Add to Calendar&lt;/a>
&lt;script type="text/javascript" src="https://cdn.addevent.com/libs/atc/1.6.1/atc.min.js" async defer>&lt;/script>
&lt;/div>
&lt;h1 id="speaker-bio">Speaker Bio&lt;/h1>
&lt;p>Artem Artemev is at his second year PhD research in Imperial College London, with Mark van der Wilk as his supervisor. Before starting PhD, Artem has worked as Machine Learning Researcher and Engineer for start-up companies. His primary interest lies in machine learning theory and approximate Bayesian inference, particularly non-parametric machine learning methods like Gaussian processes. His recent work addresses the scalability of Gaussian processes while retaining the predictive properties of that model when approximations are involved.&lt;/p></description></item></channel></rss>