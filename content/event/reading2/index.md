---
title: "Reading Group Session #2"

location: Huxley 315 - Imperial College London
#address:
#  postcode: 'Room 145'

summary: Gato (arXiv'22)
abstract: "We discuss Gato, a new generalist model and its implications for systems designers."

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2022-06-23T10:30:00Z'
date_end: '2022-06-23T11:30:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: '2022-06-19T00:00:00Z'

authors: []
profile: false
tags: ["Transformers", "DL Training", "Multi-Modal", "Multi-Task", "AGI", "Embeddings"]
categories: ['Reading Group']

# Is this a featured talk? (true/false)
featured: true

image:
  caption: 'Image credit: [**DeepMind**](https://www.deepmind.com/publications/a-generalist-agent)'
  focal_point: Center

header:
  image: "featured.png"
  caption: 'Image credit: [**DeepMind**](https://www.deepmind.com/publications/a-generalist-agent)'

url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides:

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:



# <h5 style="text-align: center;">Share</h5>
---
{{% cta cta_link="https://teams.microsoft.com/l/meetup-join/19%3aLiP6Evh3ssJQ3g41q8vAsBNwwOFTzp7d_qq1y7oTo7A1%40thread.tacv2/1654597085925?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%228d261fd5-be8f-44eb-b630-d5b230fc5ec3%22%7d" cta_text="Join Session!" %}}

# Papers

[A Generalist Agent](https://arxiv.org/pdf/2205.06175.pdf?fs=e&s=cl), arXiv'22

**Paper Abstract:**
Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.


# Discussion Notes

Please come back when this is filled in with discussion notes.


