<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Recent &amp; Upcoming Events | SysML@ICL</title><link>https://sysml-icl.github.io/event/</link><atom:link href="https://sysml-icl.github.io/event/index.xml" rel="self" type="application/rss+xml"/><description>Recent &amp; Upcoming Events</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 23 Jun 2022 10:30:00 +0000</lastBuildDate><image><url>https://sysml-icl.github.io/media/logo_hu0e4a96b537c97ee56dff564995bc792a_102878_300x300_fit_lanczos_3.png</url><title>Recent &amp; Upcoming Events</title><link>https://sysml-icl.github.io/event/</link></image><item><title>Reading Group Session #2</title><link>https://sysml-icl.github.io/event/reading2/</link><pubDate>Thu, 23 Jun 2022 10:30:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/reading2/</guid><description>&lt;ul class="cta-group">
&lt;li>
&lt;a href="https://teams.microsoft.com/l/meetup-join/19%3aLiP6Evh3ssJQ3g41q8vAsBNwwOFTzp7d_qq1y7oTo7A1%40thread.tacv2/1654597085925?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%228d261fd5-be8f-44eb-b630-d5b230fc5ec3%22%7d" class="btn btn-primary px-3 py-3">Join Session!&lt;/a>
&lt;/li>
&lt;/ul>
&lt;h1 id="papers">Papers&lt;/h1>
&lt;p>&lt;a href="https://arxiv.org/pdf/2205.06175.pdf?fs=e&amp;amp;s=cl" target="_blank" rel="noopener">A Generalist Agent&lt;/a>, arXiv'22&lt;/p>
&lt;p>&lt;strong>Paper Abstract:&lt;/strong>
Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.&lt;/p>
&lt;h1 id="discussion-notes">Discussion Notes&lt;/h1>
&lt;p>Please come back when this is filled in with discussion notes.&lt;/p></description></item><item><title>Seminar #1 - TBD</title><link>https://sysml-icl.github.io/event/seminar1/</link><pubDate>Thu, 16 Jun 2022 10:30:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/seminar1/</guid><description>&lt;ul class="cta-group">
&lt;li>
&lt;a href="https://teams.microsoft.com/l/meetup-join/19%3aLiP6Evh3ssJQ3g41q8vAsBNwwOFTzp7d_qq1y7oTo7A1%40thread.tacv2/1654597085925?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%228d261fd5-be8f-44eb-b630-d5b230fc5ec3%22%7d" class="btn btn-primary px-3 py-3">Join Session!&lt;/a>
&lt;/li>
&lt;/ul>
&lt;p>TBD Placeholder&lt;/p>
&lt;hr>
&lt;h5 style="text-align: center;">Share&lt;/h5></description></item><item><title>Reading Group Session #1</title><link>https://sysml-icl.github.io/event/reading1/</link><pubDate>Thu, 09 Jun 2022 10:30:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/reading1/</guid><description>&lt;ul class="cta-group">
&lt;li>
&lt;a href="https://teams.microsoft.com/l/meetup-join/19%3aLiP6Evh3ssJQ3g41q8vAsBNwwOFTzp7d_qq1y7oTo7A1%40thread.tacv2/1654597085925?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%228d261fd5-be8f-44eb-b630-d5b230fc5ec3%22%7d" class="btn btn-primary px-3 py-3">Join Session!&lt;/a>
&lt;/li>
&lt;/ul>
&lt;h1 id="papers">Papers&lt;/h1>
&lt;p>Papers covered will be:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://ieeexplore.ieee.org/iel7/71/9497774/09472938.pdf" target="_blank" rel="noopener">vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training&lt;/a>, TPDS'21&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/pdf/2201.12023" target="_blank" rel="noopener">Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning&lt;/a>, OSDI'22&lt;/li>
&lt;/ol>
&lt;h1 id="discussion-notes">Discussion Notes&lt;/h1>
&lt;p>In these discussion notes we attempt to summarize points made during the discussion on possible future directions.&lt;/p>
&lt;h2 id="vpipe">vPipe&lt;/h2>
&lt;h3 id="online-partitioning">Online partitioning&lt;/h3>
&lt;p>vPipes justification for the need for an online repartitioning algorithm is Neural Architecture Search (NAS).
Very few systems are using online partitioning nowadays. The question is, is there a need for them?
What are other motivating reasons for online partitioning?&lt;/p>
&lt;ul>
&lt;li>To deal with failures&lt;/li>
&lt;li>To support elasticity&lt;/li>
&lt;li>NAS&lt;/li>
&lt;li>Dynamic networks&lt;/li>
&lt;/ul>
&lt;p>In short, any source of dynamism in the training is justifiable.
What are other sources of dynamism in training?&lt;/p>
&lt;h3 id="pcie-usage">PCIe Usage&lt;/h3>
&lt;p>In vPipe PCIe is shared by swapping and inter-host activation communication.
The algorithm which decides the swap-recompute plan attempts to fill all the PCIe bandwidth but ignores activation communication traffic.
Is this optimal? Won&amp;rsquo;t this cause stalls by oversubscribing the PCIe bus?
Still, vPipe prioritizes inter-host activation communication.&lt;/p>
&lt;h2 id="alpa">Alpa&lt;/h2>
&lt;h3 id="device-meshes">Device Meshes&lt;/h3>
&lt;p>Alpa maps computations to a 2D mesh of devices. This seems to be due to the fact
that there are 2 layers in the device hierarchy, intra-host and inter-host,
where communication intra-host is faster than inter-host.
If there were a third level in this hierarchy, perhaps a 3D mesh of devices would make sense.&lt;/p>
&lt;h2 id="both-papers">Both papers&lt;/h2>
&lt;p>We noticed that both papers attempt to solve the NP-Complete task of parallelizing
a computational graph across several devices for optimal performance.
This task is too difficult to solve directly.
To tackle this, both papers use a decomposition approach, instead solving two subproblems optimally:&lt;/p>
&lt;ul>
&lt;li>Alpa first uses inter-op parallelism, then intra-op parallelism on each stage.&lt;/li>
&lt;li>vPipe first uses inter-op parallelism, then computes a swap/recompute plan for each stage.&lt;/li>
&lt;/ul>
&lt;p>Why can&amp;rsquo;t it be solved directly? Is it simply too large a search problem?
Are we losing something by not solving the original problem optimally?&lt;/p>
&lt;h3 id="heterogeneous-clusters">Heterogeneous Clusters&lt;/h3>
&lt;p>Heterogeneous clusters are a reality in several organizations which over time accumulate several generations of accelerators.
Furthermore, CPUs are a largely unused resource that is available.
Both papers, and other systems, generally target homogeneous clusters only.
We believe there is space for novel systems targeting Heterogeneous clusters, though the problem of partitioning and parallelizing becomes more difficult.&lt;/p></description></item><item><title>Parallel Automatic Differentiation Talk</title><link>https://sysml-icl.github.io/event/parallel-ad/</link><pubDate>Fri, 13 May 2022 11:00:00 +0000</pubDate><guid>https://sysml-icl.github.io/event/parallel-ad/</guid><description>&lt;p>&lt;strong>Talk Title:&lt;/strong> Enzyme: High-Performance, Cross-Language, and Parallel Automatic Differentiation&lt;/p>
&lt;p>&lt;a href="https://imperial-ac-uk.zoom.us/j/97378427420?pwd=VENub3h5eVJscXVTbDk5cG9sVGFMdz09" target="_blank" rel="noopener">Zoom Link&lt;/a>&lt;/p>
&lt;h2 id="author-bio">Author Bio&lt;/h2>
&lt;p>William Moses is a Ph.D. Candidate at MIT, where he also received his M.Eng in electrical engineering and computer science (EECS) and B.S. in EECS and physics. William&amp;rsquo;s research involves creating compilers and program representations that enable performance and use-case portability, thus enabling non-experts to leverage the latest in high-performance computing and ML. He is known as the lead developer of Enzyme (NeurIPS &amp;lsquo;20, SC &amp;lsquo;21), an automatic differentiation tool for LLVM capable of differentiating code in a variety of languages, after optimization, and for a variety of architectures and the lead developer of Polygeist (PACT &amp;lsquo;21), a polyhedral compiler and C++ frontend for MLIR. He has also worked on the Tensor Comprehensions framework for synthesizing high-performance GPU kernels of ML code, the Tapir compiler for parallel programs (best paper at PPoPP &amp;lsquo;17), and compilers that use machine learning to better optimize. He is a recipient of the U.S. Department of Energy Computational Science Graduate Fellowship and the Karl Taylor Compton Prize, MIT&amp;rsquo;s highest student award.&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Automatic differentiation (AD) is key to training neural networks, Bayesian inference, and scientific computing. Applying these techniques requires rewriting code in a specific machine learning framework or manually providing derivatives. This talk presents Enzyme, a high-performance automatic differentiation compiler plugin for the low-level virtual machine (LLVM) compiler capable of synthesizing gradients of programs expressed in the LLVM intermediate representation (IR). Enzyme differentiates programs in any language whose compiler targets LLVM, including C/C++, Fortran, Julia, Rust, Swift, etc., thereby providing native AD capabilities in these languages with state-of-the-art performance. Unlike traditional tools, Enzyme performs AD on optimized IR. On a combined machine-learning and scientific computing benchmark suite, AD on optimized IR achieves a geometric mean speedup of 4.2x over AD on IR before optimization. This talk will also include work that makes Enzyme the first fully automatic reverse-mode AD tool to generate gradients of existing GPU kernels. This includes new GPU and AD-specific compiler optimizations, and an algorithm ensuring correctness of high-performance parallel gradient computations. We provide a detailed evaluation of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs.&lt;/p>
&lt;hr>
&lt;h5 style="text-align: center;">Share&lt;/h5></description></item></channel></rss>